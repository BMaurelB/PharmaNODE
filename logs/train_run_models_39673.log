/opt/miniconda3/envs/NODE_test/lib/python3.12/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/miniconda3/envs/NODE_test/lib/python3.12/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib
  Referenced from: <367D4265-B20F-34BD-94EB-4F3EE47C385B> /opt/miniconda3/envs/NODE_test/lib/python3.12/site-packages/torchvision/image.so
  Reason: tried: '/opt/miniconda3/envs/NODE_test/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/miniconda3/envs/NODE_test/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/miniconda3/envs/NODE_test/lib/python3.12/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/miniconda3/envs/NODE_test/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/Users/benjaminmaurel/Documents/Training_Sanofi/notebooks_hands_on/latent_ode/run_models.py
run_models.py --niters 3000 -n 200 -s 40 -l 10 --dataset PK_Tacro --latent-ode --noise-weight 0.01 --max-t 5. --seed 1 --experiment 39673
Experiment 39673
Epoch 0010 [Test seq (cond on sampled tp)] | Loss 2494.343750 | Likelihood -2515.433594 | KL fp 4.0417 | FP STD 0.4009|
KL coef: 0.0
Train loss (one batch): 1752.8394775390625
Train CE loss (one batch): 0.0
Test MSE (raw): 0.5038
Test MSE (smoothed with alpha=0.01): 0.5038
New best smoothed test MSE: 0.5038. Saving best model to experiments/experiment_39673_best.ckpt
Test MSE group_0: 0.4659
Test MSE group_1: 0.5472
Test MSE group_2: nan
Test MSE group_3: nan
Poisson likelihood: 0.0
CE loss: 0.0
train MSE group_0: 0.3496
train MSE group_1: 0.3561
train MSE group_2: nan
train MSE group_3: nan
CE loss (TEST): 0.0
Experiment 39673
Epoch 0020 [Test seq (cond on sampled tp)] | Loss 756.511902 | Likelihood -781.164917 | KL fp 4.4296 | FP STD 0.5104|
KL coef: 0.09561792499119559
Train loss (one batch): 739.613525390625
Train CE loss (one batch): 0.0
Test MSE (raw): 0.1570
Test MSE (smoothed with alpha=0.01): 0.5004
New best smoothed test MSE: 0.5004. Saving best model to experiments/experiment_39673_best.ckpt
Test MSE group_0: 0.1330
Test MSE group_1: 0.1843
Test MSE group_2: nan
Test MSE group_3: nan
Poisson likelihood: 0.0
CE loss: 0.0
train MSE group_0: 0.1330
train MSE group_1: 0.1729
train MSE group_2: nan
train MSE group_3: nan
CE loss (TEST): 0.0
Experiment 39673
Epoch 0030 [Test seq (cond on sampled tp)] | Loss 677.012207 | Likelihood -693.661743 | KL fp 4.4909 | FP STD 0.5005|
KL coef: 0.18209306240276923
Train loss (one batch): 672.5155639648438
Train CE loss (one batch): 0.0
Test MSE (raw): 0.1395
Test MSE (smoothed with alpha=0.01): 0.4967
New best smoothed test MSE: 0.4967. Saving best model to experiments/experiment_39673_best.ckpt
Test MSE group_0: 0.1121
Test MSE group_1: 0.1708
Test MSE group_2: nan
Test MSE group_3: nan
Poisson likelihood: 0.0
CE loss: 0.0
train MSE group_0: 0.1228
train MSE group_1: 0.1508
train MSE group_2: nan
train MSE group_3: nan
CE loss (TEST): 0.0
Experiment 39673
Epoch 0040 [Test seq (cond on sampled tp)] | Loss 669.792480 | Likelihood -686.994995 | KL fp 4.5144 | FP STD 0.4898|
KL coef: 0.2602996266117198
Train loss (one batch): 627.9193115234375
Train CE loss (one batch): 0.0
Test MSE (raw): 0.1381
Test MSE (smoothed with alpha=0.01): 0.4932
New best smoothed test MSE: 0.4932. Saving best model to experiments/experiment_39673_best.ckpt
Test MSE group_0: 0.1049
Test MSE group_1: 0.1761
Test MSE group_2: nan
Test MSE group_3: nan
Poisson likelihood: 0.0
CE loss: 0.0
train MSE group_0: 0.1033
train MSE group_1: 0.1563
train MSE group_2: nan
train MSE group_3: nan
CE loss (TEST): 0.0
Experiment 39673
Epoch 0050 [Test seq (cond on sampled tp)] | Loss 634.562378 | Likelihood -652.000732 | KL fp 4.7885 | FP STD 0.3774|
KL coef: 0.3310282414303197
Train loss (one batch): 602.4717407226562
Train CE loss (one batch): 0.0
Test MSE (raw): 0.1311
Test MSE (smoothed with alpha=0.01): 0.4895
New best smoothed test MSE: 0.4895. Saving best model to experiments/experiment_39673_best.ckpt
Test MSE group_0: 0.1007
Test MSE group_1: 0.1659
Test MSE group_2: nan
Test MSE group_3: nan
Poisson likelihood: 0.0
CE loss: 0.0
train MSE group_0: 0.1016
train MSE group_1: 0.1462
train MSE group_2: nan
train MSE group_3: nan
CE loss (TEST): 0.0
Experiment 39673
Epoch 0060 [Test seq (cond on sampled tp)] | Loss 639.699463 | Likelihood -649.084961 | KL fp 4.7898 | FP STD 0.3460|
KL coef: 0.39499393286246365
Train loss (one batch): 580.9243774414062
Train CE loss (one batch): 0.0
Test MSE (raw): 0.1306
Test MSE (smoothed with alpha=0.01): 0.4860
New best smoothed test MSE: 0.4860. Saving best model to experiments/experiment_39673_best.ckpt
Test MSE group_0: 0.1015
Test MSE group_1: 0.1638
Test MSE group_2: nan
Test MSE group_3: nan
Poisson likelihood: 0.0
CE loss: 0.0
train MSE group_0: 0.0986
train MSE group_1: 0.1390
train MSE group_2: nan
train MSE group_3: nan
CE loss (TEST): 0.0
Experiment 39673
Epoch 0070 [Test seq (cond on sampled tp)] | Loss 627.888611 | Likelihood -637.661743 | KL fp 4.7610 | FP STD 0.3247|
KL coef: 0.4528433576092388
Train loss (one batch): 579.7501220703125
Train CE loss (one batch): 0.0
Test MSE (raw): 0.1283
Test MSE (smoothed with alpha=0.01): 0.4824
New best smoothed test MSE: 0.4824. Saving best model to experiments/experiment_39673_best.ckpt
Test MSE group_0: 0.1007
Test MSE group_1: 0.1598
Test MSE group_2: nan
Test MSE group_3: nan
Poisson likelihood: 0.0
CE loss: 0.0
train MSE group_0: 0.0984
train MSE group_1: 0.1387
train MSE group_2: nan
train MSE group_3: nan
CE loss (TEST): 0.0
Experiment 39673
Epoch 0080 [Test seq (cond on sampled tp)] | Loss 613.766968 | Likelihood -628.529297 | KL fp 4.6786 | FP STD 0.3580|
KL coef: 0.505161340399793
Train loss (one batch): 555.2593994140625
Train CE loss (one batch): 0.0
Test MSE (raw): 0.1264
Test MSE (smoothed with alpha=0.01): 0.4788
New best smoothed test MSE: 0.4788. Saving best model to experiments/experiment_39673_best.ckpt
Test MSE group_0: 0.0985
Test MSE group_1: 0.1584
Test MSE group_2: nan
Test MSE group_3: nan
Poisson likelihood: 0.0
CE loss: 0.0
train MSE group_0: 0.0950
train MSE group_1: 0.1341
train MSE group_2: nan
train MSE group_3: nan
CE loss (TEST): 0.0
Experiment 39673
Epoch 0090 [Test seq (cond on sampled tp)] | Loss 612.855835 | Likelihood -623.708252 | KL fp 4.7721 | FP STD 0.3447|
KL coef: 0.5524767862361895
Train loss (one batch): 563.6676025390625
Train CE loss (one batch): 0.0
Test MSE (raw): 0.1255
Test MSE (smoothed with alpha=0.01): 0.4753
New best smoothed test MSE: 0.4753. Saving best model to experiments/experiment_39673_best.ckpt
Test MSE group_0: 0.0964
Test MSE group_1: 0.1588
Test MSE group_2: nan
Test MSE group_3: nan
Poisson likelihood: 0.0
CE loss: 0.0
train MSE group_0: 0.0940
train MSE group_1: 0.1399
train MSE group_2: nan
train MSE group_3: nan
CE loss (TEST): 0.0
Experiment 39673
Epoch 0100 [Test seq (cond on sampled tp)] | Loss 600.489746 | Likelihood -606.972961 | KL fp 4.8410 | FP STD 0.2848|
KL coef: 0.5952680273216762
Train loss (one batch): 556.5512084960938
Train CE loss (one batch): 0.0
Test MSE (raw): 0.1221
Test MSE (smoothed with alpha=0.01): 0.4717
New best smoothed test MSE: 0.4717. Saving best model to experiments/experiment_39673_best.ckpt
Test MSE group_0: 0.0981
Test MSE group_1: 0.1497
Test MSE group_2: nan
Test MSE group_3: nan
Poisson likelihood: 0.0
CE loss: 0.0
train MSE group_0: 0.0981
train MSE group_1: 0.1280
train MSE group_2: nan
train MSE group_3: nan
CE loss (TEST): 0.0
Experiment 39673
Epoch 0110 [Test seq (cond on sampled tp)] | Loss 587.593323 | Likelihood -598.669312 | KL fp 4.7590 | FP STD 0.2839|
KL coef: 0.6339676587267709
Train loss (one batch): 543.2732543945312
Train CE loss (one batch): 0.0
Test MSE (raw): 0.1205
Test MSE (smoothed with alpha=0.01): 0.4682
New best smoothed test MSE: 0.4682. Saving best model to experiments/experiment_39673_best.ckpt
Test MSE group_0: 0.0937
Test MSE group_1: 0.1511
Test MSE group_2: nan
Test MSE group_3: nan
Poisson likelihood: 0.0
CE loss: 0.0
train MSE group_0: 0.0905
train MSE group_1: 0.1304
train MSE group_2: nan
train MSE group_3: nan
CE loss (TEST): 0.0
Experiment 39673
Epoch 0120 [Test seq (cond on sampled tp)] | Loss 567.490479 | Likelihood -576.706543 | KL fp 4.7963 | FP STD 0.2643|
KL coef: 0.6689669116789861
Train loss (one batch): 534.5584106445312
Train CE loss (one batch): 0.0
Test MSE (raw): 0.1161
Test MSE (smoothed with alpha=0.01): 0.4647
New best smoothed test MSE: 0.4647. Saving best model to experiments/experiment_39673_best.ckpt
Test MSE group_0: 0.0902
Test MSE group_1: 0.1457
Test MSE group_2: nan
Test MSE group_3: nan
Poisson likelihood: 0.0
CE loss: 0.0
train MSE group_0: 0.0895
train MSE group_1: 0.1272
train MSE group_2: nan
train MSE group_3: nan
CE loss (TEST): 0.0
Experiment 39673
Epoch 0130 [Test seq (cond on sampled tp)] | Loss 537.250977 | Likelihood -549.172363 | KL fp 4.6116 | FP STD 0.2887|
KL coef: 0.7006196086876687
Train loss (one batch): 500.362548828125
Train CE loss (one batch): 0.0
Test MSE (raw): 0.1106
Test MSE (smoothed with alpha=0.01): 0.4612
New best smoothed test MSE: 0.4612. Saving best model to experiments/experiment_39673_best.ckpt
Test MSE group_0: 0.0879
Test MSE group_1: 0.1366
Test MSE group_2: nan
Test MSE group_3: nan
Poisson likelihood: 0.0
CE loss: 0.0
train MSE group_0: 0.0852
train MSE group_1: 0.1174
train MSE group_2: nan
train MSE group_3: nan
CE loss (TEST): 0.0
Experiment 39673
Epoch 0140 [Test seq (cond on sampled tp)] | Loss 441.407196 | Likelihood -452.689697 | KL fp 4.3954 | FP STD 0.2942|
KL coef: 0.729245740488006
Train loss (one batch): 439.2882995605469
Train CE loss (one batch): 0.0
Test MSE (raw): 0.0913
Test MSE (smoothed with alpha=0.01): 0.4575
New best smoothed test MSE: 0.4575. Saving best model to experiments/experiment_39673_best.ckpt
Test MSE group_0: 0.0777
Test MSE group_1: 0.1070
Test MSE group_2: nan
Test MSE group_3: nan
Poisson likelihood: 0.0
CE loss: 0.0
train MSE group_0: 0.0802
train MSE group_1: 0.1001
train MSE group_2: nan
train MSE group_3: nan
CE loss (TEST): 0.0
Experiment 39673
Epoch 0150 [Test seq (cond on sampled tp)] | Loss 333.446716 | Likelihood -344.319397 | KL fp 4.3539 | FP STD 0.2796|
KL coef: 0.7551347009650705
Train loss (one batch): 324.0051574707031
Train CE loss (one batch): 0.0
Test MSE (raw): 0.0696
Test MSE (smoothed with alpha=0.01): 0.4536
New best smoothed test MSE: 0.4536. Saving best model to experiments/experiment_39673_best.ckpt
Test MSE group_0: 0.0558
Test MSE group_1: 0.0853
Test MSE group_2: nan
Test MSE group_3: nan
Poisson likelihood: 0.0
CE loss: 0.0
train MSE group_0: 0.0574
train MSE group_1: 0.0750
train MSE group_2: nan
train MSE group_3: nan
CE loss (TEST): 0.0
Experiment 39673
Epoch 0160 [Test seq (cond on sampled tp)] | Loss 277.283569 | Likelihood -282.122864 | KL fp 4.4585 | FP STD 0.2823|
KL coef: 0.7785482127611391
Train loss (one batch): 332.58966064453125
Train CE loss (one batch): 0.0
Test MSE (raw): 0.0572
Test MSE (smoothed with alpha=0.01): 0.4496
New best smoothed test MSE: 0.4496. Saving best model to experiments/experiment_39673_best.ckpt
Test MSE group_0: 0.0466
Test MSE group_1: 0.0692
Test MSE group_2: nan
Test MSE group_3: nan
Poisson likelihood: 0.0
CE loss: 0.0
train MSE group_0: 0.0598
train MSE group_1: 0.0762
train MSE group_2: nan
train MSE group_3: nan
CE loss (TEST): 0.0
Couldn't import umap
Sampling dataset of 200 training examples
############## ['203', '204', '205', '207', '208', '209', '210', '211', '213', '214', '216', '217', '218', '219', '220', '221', '222', '224', '225', '226', '227', '228', '229', '230', '232', '233', '234', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '267', '269', '270', '271', '272', '273', '274', '275', '276', '278', '279', '281', '282', '283', '286', '287', '288', '289', '291', '292', '293', '294', '297', '298', '299', '300', '301', '302', '303', '304', '305', '307', '308', '310', '313', '314', '315', '316', '319', '320', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '388', '389', '390', '392', '393', '394', '395', '396', '397', '398', '399', '400', '402', '403', '404', '405', '406', '407', '408', '410', '411', '412', '413', '414', '415', '416', '417', '419', '420', '421', '422', '423', '424', '425', '426', '427', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '481', '482', '483', '485', '486', '487', '488', '490', '491', '493', '494', '495', '496', '497', '498', '499', '501', '503', '504', '505', '506', '507', '508', '509', '510', '511', '513', '514', '515', '516', '518', '519', '520', '521', '522', '523', '524', '525', '527', '528', '529', '530', '531', '532', '533', '534', '535', '537', '538', '539', '540', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '554', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '615', '616', '617', '619', '620', '621', '622', '623', '624', '625', '626', '627', '628', '630', '631', '632', '633', '634', '635', '636', '637', '638', '639', '640', '641', '642', '643', '644', '645', '646', '647', '648', '649', '650', '651', '653', '654', '656', '657', '658', '659', '660', '661', '662', '664', '665', '666', '667', '668', '669', '670', '671', '672', '673', '674', '675', '676', '677', '678', '679', '680', '681', '682', '683', '684', '685', '686', '688', '690', '691', '692', '695', '696', '697', '698', '699', '700', '702', '704', '705', '706', '707', '709', '710', '711', '712', '713', '714', '715', '716', '717', '718', '719', '720', '721', '722', '723', '725', '726', '727', '728', '729', '731', '732', '733', '734', '735', '736', '737', '738', '739', '740', '741', '743', '744', '745', '746', '747', '748', '749', '750', '751', '752', '753', '754', '755', '756', '757', '759', '760', '761', '762', '763', '764', '765', '766', '767', '768', '770', '771', '772', '774', '775', '779', '780', '781', '782', '783', '784', '785', '786', '787', '789', '790', '791', '792', '793', '794', '795', '796', '797', '798', '799', '800', '801', '803', '804', '805', '806', '807', '808', '809', '810', '811', '812', '813', '814', '815', '816', '817', '818', '820', '821', '822', '823', '824', '825', '827', '828', '829', '830', '831', '832', '833', '834', '835', '836', '837', '838', '839', '840', '841', '842', '843', '844', '845', '846', '847', '848', '849', '850', '851', '852', '853', '854', '855', '856', '858', '859', '860', '862', '863', '864', '865', '866', '867', '868', '869', '870', '871', '873', '874', '875', '876', '877', '879', '880', '881', '883', '884', '885', '886', '887', '889', '890', '891', '892', '893', '894', '895', '896', '897', '898', '899', '900', '901', '902', '903', '904', '906', '907', '908', '909', '910', '911', '912', '913', '914', '915', '916', '918', '919', '920', '921', '922', '923', '924', '925', '926', '927', '928', '929', '930', '931', '932', '933', '934', '935', '937', '938', '939', '940', '941', '942', '944', '945', '946', '947', '948', '949', '950', '951', '952', '953', '954', '956', '957', '958', '959', '960', '961', '962', '963', '964', '965', '966', '967', '968', '970', '973', '974', '975', '976', '977', '978', '979', '980', '981', '982', '983', '984', '985', '986', '987', '988', '989', '990', '991', '992', '994', '995', '996', '997', '998', '999', '1000']
886
TensorBoard logs will be saved to: experiments/runs/39673
Traceback (most recent call last):
  File "/Users/benjaminmaurel/Documents/Training_Sanofi/notebooks_hands_on/latent_ode/run_models.py", line 296, in <module>
    test_res = compute_loss_all_batches(model, 
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/benjaminmaurel/Documents/Training_Sanofi/notebooks_hands_on/latent_ode/lib/utils.py", line 608, in compute_loss_all_batches
    results  = model.compute_all_losses(batch_dict,n_traj_samples = n_traj_samples, kl_coef = kl_coef, max_out = data_obj['max_out'])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/benjaminmaurel/Documents/Training_Sanofi/notebooks_hands_on/latent_ode/lib/base_models.py", line 261, in compute_all_losses
    pred_y, info = self.get_reconstruction(batch_dict["tp_to_predict"], 
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/benjaminmaurel/Documents/Training_Sanofi/notebooks_hands_on/latent_ode/lib/latent_ode.py", line 98, in get_reconstruction
    sol_y = self.diffeq_solver(first_point_enc_aug, time_steps_to_predict)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/NODE_test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/NODE_test/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/benjaminmaurel/Documents/Training_Sanofi/notebooks_hands_on/latent_ode/lib/diffeq_solver.py", line 40, in forward
    pred_y = odeint(self.ode_func, first_point, time_steps_to_predict, 
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/NODE_test/lib/python3.12/site-packages/torchdiffeq/_impl/odeint.py", line 80, in odeint
    solution = solver.integrate(t)
               ^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/NODE_test/lib/python3.12/site-packages/torchdiffeq/_impl/solvers.py", line 34, in integrate
    solution[i] = self._advance(t[i])
                  ^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/NODE_test/lib/python3.12/site-packages/torchdiffeq/_impl/rk_common.py", line 246, in _advance
    self.rk_state = self._adaptive_step(self.rk_state)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/NODE_test/lib/python3.12/site-packages/torchdiffeq/_impl/rk_common.py", line 311, in _adaptive_step
    y1, f1, y1_error, k = _runge_kutta_step(self.func, y0, f0, t0, dt, t1, tableau=self.tableau)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/NODE_test/lib/python3.12/site-packages/torchdiffeq/_impl/rk_common.py", line 79, in _runge_kutta_step
    k = _UncheckedAssign.apply(k, f, (..., i + 1))
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/envs/NODE_test/lib/python3.12/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
